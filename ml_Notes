## Scikit-learn is a popular machine learning library in Python that provides a wide range of machine learning models and algorithms. Here is a brief description of some of the most commonly used models in scikit-learn and their applications:

-  Linear Regression:
 Used for regression tasks where the relationship between the input variables and the output variable is linear.
    - example: it can be used to quantify the relative impacts of age, gender, and diet (the predictor variables) on height (the outcome variable).

- Logistic Regression: 
Used for classification tasks where the output variable is categorical.
    - example : Email Filter spam or  notspam , Fail or pass, O or 1 yes or not

-  Support Vector Machines (SVM): Used for both classification and regression tasks, particularly when the data has complex boundaries.
    - one of the most popular Supervised Learning algorithms,Classification as well as Regression problems
    - lot of training data 

- K-Nearest Neighbors (KNN): K-NN algorithm stores all the available data and classifies a new data point based on the similarity.
    - K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.
    - stores all the available data and classifies a new data point based on the similarity

- Naive Bayes: Used for classification tasks where the input variables are independent and the output variable is categorical.
    - It is mainly used in text,image classification that includes a high-dimensional training dataset.
    - example: spam filtration, Sentimental analysis, and classifying articles.

- Decision Trees: Used for both classification and regression tasks where the data has a hierarchical structure.
    - A decision tree simply asks a question, and based on the answer (Yes/No), it further split the tree into subtrees.
    - 

- Random Forest: Used for classification and regression tasks where the data has many dimensions and is prone to overfitting.
    - divdide in multiple group and give output
    - matching all grup anser in the end and then return anser 
    - Example : Banking: Banking sector mostly uses this algorithm for the identification of loan risk.

- Neural Networks: Artificial Neural Network is biologically inspired by the neural network, which constitutes after the human brain..

- Clustering Algorithms: Used for unsupervised learning tasks where the goal is to find patterns in the data without prior knowledge of the output variable.
    - A way of grouping the data points into different clusters, consisting of similar data points. The objects with the possible similarities remain in a group that has less or no similarities with another group. 
    - EXAMPLE :
            Market Segmentation
            Statistical data analysis
            social network analysis
            Image segmentation
        It is used by the Amazon in its recommendation system to provide the recommendations as per the past search of products. Netflix also uses this technique to recommend the movies and web-series to its users as per the watch history.


## modules available in scikit-learn:

- sklearn.cluster: 
    Clustering algorithms such as K-Means, DBSCAN, and Spectral Clustering.

- sklearn.compose: 
    Tools for combining and transforming features.

- sklearn.covariance:
    Estimation of covariance matrices.

- sklearn.cross_decomposition: 
    Cross decomposition methods such as PLS, CCA, and CPCA.

- sklearn.datasets: 
    A collection of toy datasets.

- sklearn.decomposition: 
    Matrix factorization techniques such as PCA, NMF, and ICA.

- sklearn.discriminant_analysis:
    Linear and Quadratic Discriminant Analysis.

- sklearn.ensemble: 
    Ensemble methods such as Random Forest, AdaBoost, and Gradient Boosting.

- sklearn.experimental: 
    Experimental features that are not yet stable.

- sklearn.feature_extraction: 
    Feature extraction methods such as Bag-of-Words, Image features, and Text features.

- sklearn.feature_selection: 
    Feature selection algorithms such as SelectKBest, SelectFromModel, and RFE.

- sklearn.gaussian_process:
    Gaussian Process models.

- sklearn.impute: 
    Imputation of missing values.

- sklearn.inspection: 
    Tools for model inspection and interpretation.

- sklearn.isotonic: 
    Isotonic regression.

- sklearn.kernel_approximation: 
    Approximate feature maps for kernel methods.

- sklearn.kernel_ridge: 
    Kernel Ridge Regression.

- sklearn.linear_model: 
Linear models such as Linear Regression, Logistic Regression, and Ridge Regression.

- sklearn.manifold:
    Manifold learning methods such as t-SNE, Isomap, and LLE.

- sklearn.metrics: 
    Metrics for evaluating model performance such as Accuracy, F1 score, and ROC-AUC.

- sklearn.mixture: 
    Gaussian Mixture Models and Bayesian Gaussian Mixture Models.

- sklearn.model_selection: 
    Tools for model selection and hyperparameter tuning such as Grid Search, Randomized Search, and Cross Validation.

- sklearn.multiclass: 
    Tools for multi-class classification.

- sklearn.multioutput: 
    Tools for multi-output regression and classification.

- sklearn.naive_bayes: 
    Naive Bayes classifiers.

- sklearn.neighbors: 
    Nearest Neighbors methods such as KNN and Radius Neighbors.

- sklearn.neural_network: 
Multi-layer Perceptron models.

- sklearn.pipeline: 
    Tools for building and evaluating machine learning pipelines.

- sklearn.preprocessing: 
    Preprocessing tools such as StandardScaler, MinMaxScaler, and LabelEncoder.

- sklearn.random_projection: 
    Random projection methods.

- sklearn.semi_supervised: 
    Semi-supervised learning methods.

- sklearn.svm: 
    Support Vector Machines and kernel methods.

- sklearn.tree: 
    Decision tree algorithms such as Classification and Regression Trees (CART), Random Forest, and Gradient Boosted Trees.

- sklearn.utils: 
    Utilities for machine learning such as Bunch, ShuffleSplit, and Resampling.